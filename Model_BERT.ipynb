{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Heui1z3IjZh_"
   },
   "source": [
    "## Authentic User Detection on Social Media\n",
    "\n",
    "With social media platforms gradually replacing dedicated news outlets and also acting as an instrument to shape public opinion about important issues, hate speech accounts and bot users are becoming increasingly common in these platforms. The objective of this task is to identify comments that were likely made by such hate speech accounts or bot users and in doing so, the patterns that distinguish a genuine user from them.\n",
    "\n",
    "The data used for this project is a collection of toxic and hateful comments found on popular social media platforms like instagram. These comments were chosen as they were toxic, racist, sexist, and were phrased in a way that was suspiciously likely to trigger people. The comments picked had to not contain information specific to a post, had to be as generic as possible, and were not questions or rhetorical. Some comments were manually added with high confidence that they were made by a hate speech account or bot user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ltte0Z-vD0u"
   },
   "source": [
    "## Annotation Results:\n",
    "Seeing as the task of classifying text as generated by bot/dedicated hate-speech account or authentic user is highly subjective, there is no fixed metric or external tool (other than another LLM) that can confidently differentiate the comments. Therefore, two human users have annoted the dataset with the labels \"Bot\" and \"Real\", which are used to denote percieved bot and authentic user generated comments respectively.\n",
    "\n",
    "We compute the inter-annotator agreement between the two annotators (i.e. the two separate files). This is done by computing both the raw agreement (% of examples for which both annotators agreed on the label) and Cohen's Kappa.\n",
    "\n",
    "*Cohen suggested the Kappa result be interpreted as follows: values ≤ 0 as indicating no agreement and 0.01–0.20 as none to slight, 0.21–0.40 as fair, 0.41– 0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1.00 as almost perfect agreement.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UD7yvkwgy82S",
    "outputId": "5e684147-44c4-4fdd-d4a5-854d33d8a3d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Agreement: 0.744\n",
      "Cohen's Kappa: 0.48888320981344235\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "dataset1 = pd.read_csv('annotator1.csv')\n",
    "dataset2 = pd.read_csv('annotator2.csv')\n",
    "\n",
    "labels1 = dataset1['Label']\n",
    "labels2 = dataset2['Label']\n",
    "\n",
    "raw_agreement = sum(labels1 == labels2) / len(labels1)\n",
    "\n",
    "cohen_kappa = cohen_kappa_score(labels1, labels2)\n",
    "\n",
    "print(\"Raw Agreement:\", raw_agreement)\n",
    "print(\"Cohen's Kappa:\", cohen_kappa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d23zfO_ALKeB"
   },
   "source": [
    "# Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N25dvF4jvYoy"
   },
   "source": [
    "Since we're dealing with large models, the first step is to change to a GPU runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edOh9ooiIW1B",
    "outputId": "6a112636-aa2f-4ece-bfda-13142cc84cef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found device: Tesla T4, n_gpu: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "device_name = torch.cuda.get_device_name()\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(f\"Found device: {device_name}, n_gpu: {n_gpu}\")\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrvH7xx9LnMC"
   },
   "source": [
    "## Installing Hugging Face's Transformers library\n",
    "Using Hugging Face's Transformers (https://github.com/huggingface/transformers), an open-source library that provides general-purpose architectures for natural language understanding and generation with a collection of various pretrained models made by the NLP community. This library will allowe the easy use pretrained models like `BERT` and perform experiments on top of them. These models can be used to solve downstream target tasks, such as text classification, question answering, and sequence labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gtqS2e5fxpqa",
    "outputId": "847a401d-dc29-44ec-db82-86db61b5aa1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success!\n",
      "helper file downloaded! (helpers.py)\n",
      "sample tweets downloaded! (tweets.csv)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install -U -q PyDrive\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "print('success!')\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Download helper functions file\n",
    "helper_file = drive.CreateFile({'id': '16HW-z9Y1tM3gZ_vFpJAuwUDohz91Aac-'})\n",
    "helper_file.GetContentFile('helpers.py')\n",
    "print('helper file downloaded! (helpers.py)')\n",
    "\n",
    "# Download sample file of tweets\n",
    "data_file = drive.CreateFile({'id': '1QcoAmjOYRtsMX7njjQTYooIbJHPc6Ese'})\n",
    "data_file.GetContentFile('tweets.csv')\n",
    "print('sample tweets downloaded! (tweets.csv)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8XIL7wPovVX"
   },
   "source": [
    "The cell below imports some helper functions we wrote to demonstrate the task on the sample tweet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Taseb33Sovg0"
   },
   "outputs": [],
   "source": [
    "from helpers import tokenize_and_format, flat_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKc0xYh-MAbc"
   },
   "source": [
    "# Data Prep and Model Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGhkeLQlNNr8",
    "outputId": "834e241d-e84d-44fc-8fc7-b6398952778d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  I'm sorry, but I can't take you seriously with that picture.\n",
      "Token IDs: tensor([ 101, 1045, 1005, 1049, 3374, 1010, 2021, 1045, 2064, 1005, 1056, 2202,\n",
      "        2017, 5667, 2007, 2008, 3861, 1012,  102,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "from helpers import tokenize_and_format, flat_accuracy\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('final_data.csv')\n",
    "#df = pd.read_csv('tweets.csv')\n",
    "\n",
    "# Replacing values in the 'Type' column\n",
    "df['Label'] = df['Label'].replace({'Bot': 0, 'Real': 1})\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "texts = df.Text.values\n",
    "labels = df.Label.values\n",
    "\n",
    "input_ids, attention_masks = tokenize_and_format(texts)\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', texts[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3D-CzQEUXYz"
   },
   "source": [
    "## Create train/test/validation splits\n",
    "\n",
    "Here we split your dataset into 3 parts: a training set, a validation set, and a testing set. Each item in the dataset will be of 3 tuples containing an input_id tensor, an attention_mask tensor, and a label tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGgeZ3M0UWs0"
   },
   "outputs": [],
   "source": [
    "\n",
    "total = len(df)\n",
    "\n",
    "num_train = int(total * .8)\n",
    "num_val = int(total * .1)\n",
    "num_test = total - num_train - num_val\n",
    "\n",
    "# make lists of 3-tuples (already shuffled the dataframe in cell above)\n",
    "\n",
    "train_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_train)]\n",
    "val_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_train, num_val+num_train)]\n",
    "test_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_val + num_train, total)]\n",
    "\n",
    "train_text = [texts[i] for i in range(num_train)]\n",
    "val_text = [texts[i] for i in range(num_train, num_val+num_train)]\n",
    "test_text = [texts[i] for i in range(num_val + num_train, total)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCr006iTkqwM"
   },
   "source": [
    "Choosing the model we want to finetune from https://huggingface.co/transformers/pretrained_models.html. As the task requires labelling sentences, I am using BertForSequenceClassification below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lPo640_ZlEPK",
    "outputId": "aa992d05-7a92-4a5e-9db1-1422c829b25f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 3, # The number of output labels.\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3lLdoW_le3M"
   },
   "source": [
    "# Fine-tuning hyperparameters\n",
    "After initially running the model with default parameters, I got a test accuracy of 0.83. Then, I lowered the batch size, epochs and learning rate to 50, 5 and 2e-5 to mitigate potential \"catastrophic forgetting\", based on this paper https://arxiv.org/abs/1905.05583. From there, I kept increasing all 3 parameters gradually (batch size in increments of 10, epochs by 1, lr set to 5e-5,4e-5, 3e-5, 2e-5). Eventually, I got a test accuracy of 91.6 using a different training and test with the batch_size=99, epochs=99, lr=5e-5.\n",
    "\n",
    "There was almost no difference between the best validation and test accuracy (both were 91.6). This is likely due to the fact that the model picked up the key patterns in the text accurately during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dd2JdC6IletV",
    "outputId": "82884f07-685d-48ab-8406-eed482b4e779"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 99\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 5e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
    "                )\n",
    "epochs = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_Mzr-kd5RaY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# function to get validation accuracy\n",
    "def get_validation_performance(val_set):\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    num_batches = int(len(val_set)/batch_size) + 1\n",
    "\n",
    "    total_correct = 0\n",
    "\n",
    "    for i in range(num_batches):\n",
    "\n",
    "      end_index = min(batch_size * (i+1), len(val_set))\n",
    "\n",
    "      batch = val_set[i*batch_size:end_index]\n",
    "\n",
    "      if len(batch) == 0: continue\n",
    "\n",
    "      input_id_tensors = torch.stack([data[0] for data in batch])\n",
    "      input_mask_tensors = torch.stack([data[1] for data in batch])\n",
    "      label_tensors = torch.stack([data[2] for data in batch])\n",
    "\n",
    "      # Move tensors to the GPU\n",
    "      b_input_ids = input_id_tensors.to(device)\n",
    "      b_input_mask = input_mask_tensors.to(device)\n",
    "      b_labels = label_tensors.to(device)\n",
    "\n",
    "      with torch.no_grad():\n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        outputs = model(b_input_ids,\n",
    "                                token_type_ids=None,\n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the number of correctly labeled examples in batch\n",
    "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "        labels_flat = label_ids.flatten()\n",
    "        num_correct = np.sum(pred_flat == labels_flat)\n",
    "        total_correct += num_correct\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_correct / len(val_set)\n",
    "    return avg_val_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HTf_ipbjWNoV",
    "outputId": "d6df743f-2505-4743-8345-5301aa888a1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 9 ========\n",
      "Training...\n",
      "Total loss: 0.015111084096133709\n",
      "Validation accuracy: 0.75\n",
      "\n",
      "======== Epoch 2 / 9 ========\n",
      "Training...\n",
      "Total loss: 0.01311499997973442\n",
      "Validation accuracy: 0.75\n",
      "\n",
      "======== Epoch 3 / 9 ========\n",
      "Training...\n",
      "Total loss: 0.012720595113933086\n",
      "Validation accuracy: 0.75\n",
      "\n",
      "======== Epoch 4 / 9 ========\n",
      "Training...\n",
      "Total loss: 0.011224128305912018\n",
      "Validation accuracy: 0.75\n",
      "\n",
      "======== Epoch 5 / 9 ========\n",
      "Training...\n",
      "Total loss: 0.010882757604122162\n",
      "Validation accuracy: 0.75\n",
      "\n",
      "======== Epoch 6 / 9 ========\n",
      "Training...\n",
      "Total loss: 0.010216883383691311\n",
      "Validation accuracy: 0.75\n",
      "\n",
      "======== Epoch 7 / 9 ========\n",
      "Training...\n",
      "Total loss: 0.009019187651574612\n",
      "Validation accuracy: 0.75\n",
      "\n",
      "======== Epoch 8 / 9 ========\n",
      "Training...\n",
      "Total loss: 0.008671858347952366\n",
      "Validation accuracy: 0.75\n",
      "\n",
      "======== Epoch 9 / 9 ========\n",
      "Training...\n",
      "Total loss: 0.008160402067005634\n",
      "Validation accuracy: 0.75\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# training loop\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    num_batches = int(len(train_set)/batch_size) + 1\n",
    "\n",
    "    for i in range(num_batches):\n",
    "      end_index = min(batch_size * (i+1), len(train_set))\n",
    "\n",
    "      batch = train_set[i*batch_size:end_index]\n",
    "\n",
    "      if len(batch) == 0: continue\n",
    "\n",
    "      input_id_tensors = torch.stack([data[0] for data in batch])\n",
    "      input_mask_tensors = torch.stack([data[1] for data in batch])\n",
    "      label_tensors = torch.stack([data[2] for data in batch])\n",
    "\n",
    "      # Move tensors to the GPU\n",
    "      b_input_ids = input_id_tensors.to(device)\n",
    "      b_input_mask = input_mask_tensors.to(device)\n",
    "      b_labels = label_tensors.to(device)\n",
    "\n",
    "      # Clear the previously calculated gradient\n",
    "      model.zero_grad()\n",
    "\n",
    "      # Perform a forward pass (evaluate the model on this training batch).\n",
    "      outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "      loss = outputs.loss\n",
    "      logits = outputs.logits\n",
    "\n",
    "      total_train_loss += loss.item()\n",
    "\n",
    "      # Perform a backward pass to calculate the gradients.\n",
    "      loss.backward()\n",
    "\n",
    "      # Update parameters and take a step using the computed gradient.\n",
    "      optimizer.step()\n",
    "\n",
    "    print(f\"Total loss: {total_train_loss}\")\n",
    "    val_acc = get_validation_performance(val_set)\n",
    "    print(f\"Validation accuracy: {val_acc}\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9DpRJE5mHkO"
   },
   "source": [
    "# Evaluating the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "msvZ78ii3cZZ",
    "outputId": "6d245cfc-30d1-4d1b-9af5-502cad965bec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_validation_performance(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBbdMwt79fIs"
   },
   "source": [
    "## Error Analysis\n",
    "Here, the model is tested on a few examples of comments that have been intentionally phrased or designed in a way that it would be difficult for any model trained on even a larger dataset to distinguish as bot generated or authentic user generated (i.e. adversarial examples).\n",
    "\n",
    "## Note:\n",
    "This version of the project's code has removed the input comments in the code and the output for this task, as they contain examples of hate speech that I don't want to include in my GitHub repo. These parts alone have been modified where required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X72mumhI9WdR",
    "outputId": "eb945198-c86f-4da4-c48f-6d8f215b4575"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "custom_test=[\"Comment_1\",\"Comment_2\",\"etc.\"]\n",
    "inputs = tokenizer(custom_test, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "device = torch.device('cuda')\n",
    "inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "\n",
    "predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "for text, label in zip(custom_test, predicted_labels):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted label: {label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBbdMwt79fIs"
   },
   "source": [
    "## Acknowledgement\n",
    "This project is based on the [Advanced NLP Course](https://www.google.co.in/url?sa=t&source=web&rct=j&opi=89978449&url=https://people.cs.umass.edu/~miyyer/cs685/&ved=2ahUKEwiKsuiJ7c-FAxX1F1kFHXfuDrwQFnoECBMQAQ&usg=AOvVaw3J5jL9O4grGUhERJfaqCIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "SgNZTjrhcHa0"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
